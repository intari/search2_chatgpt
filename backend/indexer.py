import os
import requests
import time
import logging
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple, Set
from pdfminer.high_level import extract_text as pdf_extract_text
from pdfminer.pdfparser import PDFSyntaxError
from ebooklib import epub, ITEM_DOCUMENT
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
FILES_DIR: str = os.getenv("LOCAL_STORAGE_PATH", "/mnt/storage")
SEARCH_ENGINE_URL: str = os.getenv("MEILI_URL", "http://meilisearch:7700")
MEILI_API_KEY: Optional[str] = os.getenv("MEILI_MASTER_KEY") # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Master Key –∏–ª–∏ Index API Key
INDEX_NAME: str = "documents"
BATCH_SIZE: int = 100 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Meilisearch –∑–∞ —Ä–∞–∑

# --- –§—É–Ω–∫—Ü–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ ---

def extract_text_from_txt(file_path: Path) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ TXT —Ñ–∞–π–ª–∞, –ø—Ä–æ–±—É—è —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏."""
    encodings_to_try = ['utf-8', 'cp1251', 'latin-1']
    for encoding in encodings_to_try:
        try:
            return file_path.read_text(encoding=encoding)
        except UnicodeDecodeError:
            continue
        except Exception as e:
            raise IOError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å TXT —Ñ–∞–π–ª {file_path} –¥–∞–∂–µ –ø–æ—Å–ª–µ –ø–æ–ø—ã—Ç–æ–∫ —Å–º–µ–Ω—ã –∫–æ–¥–∏—Ä–æ–≤–∫–∏.") from e
    # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–µ –ø–æ–¥–æ—à–ª–∞
    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è TXT —Ñ–∞–π–ª–∞: {file_path.name}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
    raise ValueError(f"Unknown encoding for {file_path.name}")


def extract_text_from_pdf(file_path: Path) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞."""
    try:
        return pdf_extract_text(str(file_path))
    except PDFSyntaxError as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ PDF: {file_path.name}") from e
    except Exception as e:
        # –õ–æ–≤–∏–º –¥—Ä—É–≥–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏ pdfminer
        raise IOError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å PDF —Ñ–∞–π–ª {file_path.name}") from e

def extract_text_from_epub(file_path: Path) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ EPUB —Ñ–∞–π–ª–∞."""
    try:
        book = epub.read_epub(str(file_path))
        text_parts: List[str] = []
        for item in book.get_items_of_type(ITEM_DOCUMENT):
            soup = BeautifulSoup(item.content, "html.parser")
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏, —á—Ç–æ–±—ã –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            for script_or_style in soup(["script", "style"]):
                script_or_style.decompose()
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç, —Ä–∞–∑–¥–µ–ª—è—è –±–ª–æ–∫–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º strip=True –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ –∫—Ä–∞—è–º
            block_text = soup.get_text(separator='\n', strip=True)
            if block_text:
                text_parts.append(block_text)
        return "\n\n".join(text_parts) # –†–∞–∑–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Ä–∞–∑–Ω—ã—Ö HTML-—Ñ–∞–π–ª–æ–≤ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Å—Ç—Ä–æ–∫–∏
    except KeyError as e:
        # –ò–Ω–æ–≥–¥–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ–º –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π epub
         raise ValueError(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã EPUB —Ñ–∞–π–ª–∞: {file_path.name}, KeyError: {e}") from e
    except Exception as e:
        raise IOError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å EPUB —Ñ–∞–π–ª {file_path.name}") from e

# --- –§—É–Ω–∫—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Meilisearch ---

def get_meili_client() -> requests.Session:
    """–°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è Meilisearch."""
    session = requests.Session()
    headers = {}
    if MEILI_API_KEY:
        headers['Authorization'] = f'Bearer {MEILI_API_KEY}'
    session.headers.update(headers)
    return session

def get_indexed_files(client: requests.Session) -> Dict[str, float]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID –∏ –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ Meilisearch."""
    indexed_files: Dict[str, float] = {}
    offset = 0
    limit = 1000 # –ü–æ–ª—É—á–∞–µ–º –ø–æ 1000 –∑–∞ —Ä–∞–∑
    url = f"{SEARCH_ENGINE_URL}/indexes/{INDEX_NAME}/documents"
    params = {"limit": limit, "fields": "id,file_mtime"}

    while True:
        try:
            params["offset"] = offset
            response = client.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            results = data.get("results", [])
            if not results:
                break # –ë–æ–ª—å—à–µ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

            for doc in results:
                 # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ file_mtime —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º
                 mtime = doc.get("file_mtime")
                 if isinstance(mtime, (int, float)):
                      indexed_files[doc['id']] = float(mtime)
                 else:
                     # –ï—Å–ª–∏ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ—Ç, —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
                      indexed_files[doc['id']] = 0.0

            offset += len(results)

            # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞, –µ—Å–ª–∏ API –≤–µ—Ä–Ω–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if len(results) < limit:
                break

        except requests.exceptions.HTTPError as e:
             # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω (404), —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
            if e.response.status_code == 404:
                logger.info(f"–ò–Ω–¥–µ–∫—Å '{INDEX_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
                return {} # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å
            else:
                 logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Meilisearch: {e}")
                 raise # –ü–µ—Ä–µ–¥–∞–µ–º –æ—à–∏–±–∫—É –¥–∞–ª—å—à–µ, —Ç.–∫. –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Meilisearch ({url}): {e}")
            raise
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(indexed_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ '{INDEX_NAME}'.")
    return indexed_files

def update_meili_index(client: requests.Session, documents: List[Dict[str, Any]]) -> None:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞–∫–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Meilisearch –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    if not documents:
        return
    url = f"{SEARCH_ENGINE_URL}/indexes/{INDEX_NAME}/documents"
    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞—Å—Ç—è–º–∏ (–±–∞—Ç—á–∞–º–∏)
        for i in range(0, len(documents), BATCH_SIZE):
            batch = documents[i:i + BATCH_SIZE]
            response = client.post(url, json=batch)
            response.raise_for_status()
            task_info = response.json()
            logger.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é. Task UID: {task_info.get('taskUid', 'N/A')}")
            # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ Meilisearch
            time.sleep(0.1) # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏

    except requests.exceptions.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Meilisearch: {e}")
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏ –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ—É–¥–∞–≤—à–∏—Ö—Å—è –±–∞—Ç—á–µ–π

def delete_from_meili_index(client: requests.Session, file_ids: List[str]) -> None:
    """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Meilisearch –ø–æ —Å–ø–∏—Å–∫—É ID."""
    if not file_ids:
        return
    url = f"{SEARCH_ENGINE_URL}/indexes/{INDEX_NAME}/documents/delete-batch"
    try:
        # –£–¥–∞–ª—è–µ–º —á–∞—Å—Ç—è–º–∏ (–±–∞—Ç—á–∞–º–∏)
        for i in range(0, len(file_ids), BATCH_SIZE):
             batch_ids = file_ids[i:i + BATCH_SIZE]
             response = client.post(url, json=batch_ids)
             response.raise_for_status()
             task_info = response.json()
             logger.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(batch_ids)} ID –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ. Task UID: {task_info.get('taskUid', 'N/A')}")
             time.sleep(0.1) # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞

    except requests.exceptions.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Meilisearch: {e}")

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ ---

def process_file(file_path: Path) -> Optional[Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª: –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è Meilisearch."""
    filename = file_path.name
    file_mtime = os.path.getmtime(str(file_path))
    content = None
    file_ext = file_path.suffix.lower()

    try:
        logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filename}")
        if file_ext == ".txt":
            content = extract_text_from_txt(file_path)
        elif file_ext == ".pdf":
            content = extract_text_from_pdf(file_path)
        elif file_ext == ".epub":
            content = extract_text_from_epub(file_path)
        else:
            logger.debug(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {filename}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            return None # –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç

        if content is None or not content.strip():
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç: {filename}")
            return None

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è Meilisearch
        document = {
            "id": filename, # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∫–∞–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            "content": content.strip(),
            "file_mtime": file_mtime, # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
            "indexed_at": time.time() # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        }
        return document

    except (ValueError, IOError, Exception) as e:
        # –õ–æ–≤–∏–º –æ—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è, –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–ª–∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–∞–π–ª–æ–º
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
        return None # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Ñ–∞–π–ª

def scan_and_index_files() -> None:
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –∏–Ω–¥–µ–∫—Å–æ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç Meilisearch."""
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {FILES_DIR}")
    target_dir = Path(FILES_DIR)
    if not target_dir.is_dir():
        logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {FILES_DIR}")
        return

    client = get_meili_client()

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    try:
        indexed_files_mtimes: Dict[str, float] = get_indexed_files(client)
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞. –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ: {e}")
        return

    # 2. –°–∫–∞–Ω–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
    local_files_mtimes: Dict[str, float] = {}
    files_to_process: List[Path] = []
    processed_extensions = {".txt", ".pdf", ".epub"}

    for item in target_dir.rglob('*'): # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
        if item.is_file() and item.suffix.lower() in processed_extensions:
             try:
                  local_files_mtimes[item.name] = item.stat().st_mtime
                  files_to_process.append(item)
             except FileNotFoundError:
                 logger.warning(f"–§–∞–π–ª –±—ã–ª —É–¥–∞–ª–µ–Ω –≤–æ –≤—Ä–µ–º—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {item.name}")
                 continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —Ñ–∞–π–ª –∏—Å—á–µ–∑ –º–µ–∂–¥—É –ª–∏—Å—Ç–∏–Ω–≥–æ–º –∏ stat()

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(local_files_mtimes)} –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ.")

    # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
    local_filenames: Set[str] = set(local_files_mtimes.keys())
    indexed_filenames: Set[str] = set(indexed_files_mtimes.keys())

    files_to_add: Set[str] = local_filenames - indexed_filenames
    files_to_delete: Set[str] = indexed_filenames - local_filenames
    files_to_check_for_update: Set[str] = local_filenames.intersection(indexed_filenames)

    files_to_update: Set[str] = {
        fname for fname in files_to_check_for_update
        if local_files_mtimes[fname] > indexed_files_mtimes.get(fname, 0.0) # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
    }

    logger.info(f"–ö –¥–æ–±–∞–≤–ª–µ–Ω–∏—é: {len(files_to_add)}, –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é: {len(files_to_update)}, –∫ —É–¥–∞–ª–µ–Ω–∏—é: {len(files_to_delete)}")

    # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    docs_for_meili: List[Dict[str, Any]] = []
    files_requiring_processing: Set[str] = files_to_add.union(files_to_update)

    processed_count = 0
    skipped_count = 0
    error_count = 0

    for file_path in files_to_process:
        if file_path.name in files_requiring_processing:
            processed_count +=1
            document = process_file(file_path)
            if document:
                docs_for_meili.append(document)
            else:
                error_count += 1 # –û—à–∏–±–∫–∞ –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
        else:
             skipped_count +=1 # –§–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_count} (–ø—Ä–æ–ø—É—â–µ–Ω–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {skipped_count}, –æ—à–∏–±–∫–∏: {error_count})")

    if docs_for_meili:
        logger.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ {len(docs_for_meili)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Meilisearch...")
        update_meili_index(client, docs_for_meili)
    else:
        logger.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")

    # 5. –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if files_to_delete:
        logger.info(f"–£–¥–∞–ª–µ–Ω–∏–µ {len(files_to_delete)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Meilisearch...")
        delete_from_meili_index(client, list(files_to_delete))
    else:
        logger.info("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ –∏–Ω–¥–µ–∫—Å–∞.")

    logger.info("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


if __name__ == "__main__":
    scan_and_index_files()
